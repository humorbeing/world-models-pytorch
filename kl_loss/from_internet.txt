self.kl_loss = - 0.5 * tf.reduce_sum(
  (1 + self.logvar - tf.square(self.mu) - tf.exp(self.logvar)),
  reduction_indices = 1
)
self.kl_loss = tf.maximum(self.kl_loss, self.kl_tolerance * self.z_size)
self.kl_loss = tf.reduce_mean(self.kl_loss)



def compute_loss(self):
		logits_flat = tf.layers.flatten(self.reconstructions)
		labels_flat = tf.layers.flatten(self.resized_image)
		reconstruction_loss = tf.reduce_sum(tf.square(logits_flat - labels_flat), axis = 1)
		kl_loss = 0.5 * tf.reduce_sum(tf.exp(self.z_logvar) + self.z_mu**2 - 1. - self.z_logvar, 1)
		vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)
		return vae_loss


kl_loss = -0.5 * (1 + log_var - tf.square(mean) - tf.exp(log_var))
        kl_loss = tf.reduce_sum(kl_loss, axis=-1)
        kl_loss = tf.reduce_mean(kl_loss, axis=0, name='kl_loss')



def loss_function(recon_x, x, mu, logvar):
    n, c, h, w = recon_x.size()
    recon_x = recon_x.view(n, -1)
    x = x.view(n, -1)
    # L2 distance
    l2_dist = torch.mean(torch.sum(torch.pow(recon_x - x, 2), 1))
    # see Appendix B from VAE paper:
    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014
    # https://arxiv.org/abs/1312.6114
    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return l2_dist + KLD


def vae_loss(x, xr, mu, sigma):
    mu_sum_sq = (mu * mu).sum(dim=1)
    sig_sum_sq = (sigma * sigma).sum(dim=1)
    log_term = (1 + torch.log(sigma ** 2)).sum(dim=1)
    kldiv = -0.5 * (log_term - mu_sum_sq - sig_sum_sq)

    rec = F.mse_loss(xr, x)

    return rec + kldiv.mean()